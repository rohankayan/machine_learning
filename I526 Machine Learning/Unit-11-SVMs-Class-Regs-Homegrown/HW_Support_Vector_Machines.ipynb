{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric Guideline for  HW \n",
    "\n",
    "Rubric for each HW question: \n",
    "* Downloaded the data correctly [5points]\n",
    "* Good EDA (Exploratory data analysis) [5points]\n",
    "* Metrics defined for success  [5points]\n",
    "* Use of pipelines [5 points]\n",
    "* Data prep (split into train, and test)  [10 points]\n",
    "  * fill in missing data\n",
    "  * standardizing the data\n",
    "  * categorical pipeline\n",
    "  * numerical pipeline\n",
    "* Machine learning pipeline (for a baseline) [5 points]\n",
    "* Experimental results table shown  [5 points]\n",
    "* Bonus points \n",
    "  * statistical significance tests a baseline versus challenger [for 5 bonus points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: MNIST Handwritten Digit classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Task: train an SVM classifier on the MNIST dataset. Since SVM classifiers are binary classifiers, you will need to use one-versus-all to classify all 10 digits. You may want to tune the hyperparameters using small validation sets to speed up the process. What accuracy can you reach?_\n",
    "\n",
    "** NOTE** The solution is partially filled in. But you will need to fill in the missing parts and run all cells, report results and your conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets train  on a dataset of handwritten numbers, with labels to tell us what the numbers should be. MNIST has 60,000 training examples, and 10,000 for testing.\n",
    "\n",
    "First, let's load the dataset and split it into a training set and a test set. We could use `train_test_split()` but people usually just take the first 60,000 instances for the training set, and the last 10,000 instances for the test set (this makes it possible to compare your model's performance with others): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download MNSIT Data\n",
    "this might take a minute to download..retry if if fails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "mnist = fetch_mldata(\"MNIST original\")\n",
    "#mnist = fetch_mldata('mnist-original', data_home='d:\\\\')\n",
    "X = mnist[\"data\"]\n",
    "y = mnist[\"target\"]\n",
    "\n",
    "X_train = X[:60000]\n",
    "y_train = y[:60000]\n",
    "X_test = X[60000:]\n",
    "y_test = y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., ...,  9.,  9.,  9.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many training algorithms are sensitive to the order of the training instances, so it's generally good practice to shuffle them first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "rnd_idx = np.random.permutation(60000)\n",
    "X_train = X_train[rnd_idx]\n",
    "y_train = y_train[rnd_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a linear SVM classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start simple, with a linear SVM classifier. It will automatically use the One-vs-All (also called One-vs-the-Rest, OvR) strategy, so there's nothing special we need to do. Easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=42, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "#might take a minute or two to train\n",
    "lin_clf = LinearSVC(random_state=42)\n",
    "lin_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make predictions on the training set and measure the accuracy (we don't want to measure it on the test set yet, since we have not selected and trained the final model yet):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85375000000000001"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = lin_clf.predict(X_train)\n",
    "accuracy_score(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, 82% accuracy on MNIST is a really bad performance. This linear model is certainly too simple for MNIST, but perhaps we just needed to scale the data first:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the data and put in a pipeline\n",
    "\n",
    "Standardize the data and put in a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.astype(np.float32))\n",
    "X_test_scaled = scaler.transform(X_test.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=42, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_clf = LinearSVC(random_state=42)\n",
    "lin_clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9204"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = lin_clf.predict(X_train_scaled)\n",
    "accuracy_score(y_train,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use non-linear kernels (RBFs, Polynomial?)\n",
    "\n",
    "Put everything in a **pipeline**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's much better (we cut the error rate in two), but still not great at all for MNIST. If we want to use an SVM, we will have to use a kernel. Let's try an `SVC` with an RBF kernel (the default).\n",
    "\n",
    "**Warning**: if you are using Scikit-Learn â‰¤ 0.19, the `SVC` class will use the One-vs-One (OvO) strategy by default, so you must explicitly set `decision_function_shape=\"ovr\"` if you want to use the OvR strategy instead (OvR is the default since 0.19)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_clf = SVC(decision_function_shape=\"ovr\")\n",
    "svm_clf.fit(X_train_scaled[:10000], y_train[:10000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94615000000000005"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = svm_clf.predict(X_train_scaled)\n",
    "accuracy_score(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune hyperparameters\n",
    "\n",
    "Put everything in a **pipeline**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's promising, we get better performance even though we trained the model on 6 times less data. Let's tune the hyperparameters by doing a randomized search with cross validation. We will do this on a small dataset just to speed up the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] C=79.5231605842, gamma=0.00176607465048 .........................\n",
      "[CV] .......... C=79.5231605842, gamma=0.00176607465048, total=   0.5s\n",
      "[CV] C=79.5231605842, gamma=0.00176607465048 .........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .......... C=79.5231605842, gamma=0.00176607465048, total=   0.5s\n",
      "[CV] C=79.5231605842, gamma=0.00176607465048 .........................\n",
      "[CV] .......... C=79.5231605842, gamma=0.00176607465048, total=   0.5s\n",
      "[CV] C=2.07005900705, gamma=0.00184893979432 .........................\n",
      "[CV] .......... C=2.07005900705, gamma=0.00184893979432, total=   0.5s\n",
      "[CV] C=2.07005900705, gamma=0.00184893979432 .........................\n",
      "[CV] .......... C=2.07005900705, gamma=0.00184893979432, total=   0.5s\n",
      "[CV] C=2.07005900705, gamma=0.00184893979432 .........................\n",
      "[CV] .......... C=2.07005900705, gamma=0.00184893979432, total=   0.5s\n",
      "[CV] C=89.8773818971, gamma=0.0408025396137 ..........................\n",
      "[CV] ........... C=89.8773818971, gamma=0.0408025396137, total=   0.7s\n",
      "[CV] C=89.8773818971, gamma=0.0408025396137 ..........................\n",
      "[CV] ........... C=89.8773818971, gamma=0.0408025396137, total=   0.7s\n",
      "[CV] C=89.8773818971, gamma=0.0408025396137 ..........................\n",
      "[CV] ........... C=89.8773818971, gamma=0.0408025396137, total=   0.6s\n",
      "[CV] C=48.997015194, gamma=0.00464565003701 ..........................\n",
      "[CV] ........... C=48.997015194, gamma=0.00464565003701, total=   0.6s\n",
      "[CV] C=48.997015194, gamma=0.00464565003701 ..........................\n",
      "[CV] ........... C=48.997015194, gamma=0.00464565003701, total=   0.6s\n",
      "[CV] C=48.997015194, gamma=0.00464565003701 ..........................\n",
      "[CV] ........... C=48.997015194, gamma=0.00464565003701, total=   0.6s\n",
      "[CV] C=59.4899112775, gamma=0.0157352905643 ..........................\n",
      "[CV] ........... C=59.4899112775, gamma=0.0157352905643, total=   0.6s\n",
      "[CV] C=59.4899112775, gamma=0.0157352905643 ..........................\n",
      "[CV] ........... C=59.4899112775, gamma=0.0157352905643, total=   0.6s\n",
      "[CV] C=59.4899112775, gamma=0.0157352905643 ..........................\n",
      "[CV] ........... C=59.4899112775, gamma=0.0157352905643, total=   0.6s\n",
      "[CV] C=88.4491791678, gamma=0.0294618722484 ..........................\n",
      "[CV] ........... C=88.4491791678, gamma=0.0294618722484, total=   0.6s\n",
      "[CV] C=88.4491791678, gamma=0.0294618722484 ..........................\n",
      "[CV] ........... C=88.4491791678, gamma=0.0294618722484, total=   0.6s\n",
      "[CV] C=88.4491791678, gamma=0.0294618722484 ..........................\n",
      "[CV] ........... C=88.4491791678, gamma=0.0294618722484, total=   0.6s\n",
      "[CV] C=29.2271785963, gamma=0.00614152194782 .........................\n",
      "[CV] .......... C=29.2271785963, gamma=0.00614152194782, total=   0.6s\n",
      "[CV] C=29.2271785963, gamma=0.00614152194782 .........................\n",
      "[CV] .......... C=29.2271785963, gamma=0.00614152194782, total=   0.6s\n",
      "[CV] C=29.2271785963, gamma=0.00614152194782 .........................\n",
      "[CV] .......... C=29.2271785963, gamma=0.00614152194782, total=   0.6s\n",
      "[CV] C=28.155233269, gamma=0.0219465853153 ...........................\n",
      "[CV] ............ C=28.155233269, gamma=0.0219465853153, total=   0.6s\n",
      "[CV] C=28.155233269, gamma=0.0219465853153 ...........................\n",
      "[CV] ............ C=28.155233269, gamma=0.0219465853153, total=   0.7s\n",
      "[CV] C=28.155233269, gamma=0.0219465853153 ...........................\n",
      "[CV] ............ C=28.155233269, gamma=0.0219465853153, total=   0.6s\n",
      "[CV] C=43.2924613427, gamma=0.00374501745191 .........................\n",
      "[CV] .......... C=43.2924613427, gamma=0.00374501745191, total=   0.6s\n",
      "[CV] C=43.2924613427, gamma=0.00374501745191 .........................\n",
      "[CV] .......... C=43.2924613427, gamma=0.00374501745191, total=   0.6s\n",
      "[CV] C=43.2924613427, gamma=0.00374501745191 .........................\n",
      "[CV] .......... C=43.2924613427, gamma=0.00374501745191, total=   0.6s\n",
      "[CV] C=39.9867874863, gamma=0.0224153322878 ..........................\n",
      "[CV] ........... C=39.9867874863, gamma=0.0224153322878, total=   0.6s\n",
      "[CV] C=39.9867874863, gamma=0.0224153322878 ..........................\n",
      "[CV] ........... C=39.9867874863, gamma=0.0224153322878, total=   0.6s\n",
      "[CV] C=39.9867874863, gamma=0.0224153322878 ..........................\n",
      "[CV] ........... C=39.9867874863, gamma=0.0224153322878, total=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:   29.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score='raise',\n",
       "          estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "          fit_params={}, iid=True, n_iter=10, n_jobs=1,\n",
       "          param_distributions={'gamma': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001F4DA85A208>, 'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001F4DA85A710>},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import reciprocal, uniform\n",
    "\n",
    "param_distributions = {\"gamma\": reciprocal(0.001, 0.1), \"C\": uniform(1, 100)}\n",
    "rnd_search_cv = RandomizedSearchCV(svm_clf, param_distributions, n_iter=10, verbose=2)\n",
    "rnd_search_cv.fit(X_train_scaled[:1000], y_train[:1000])  #### use a subset of the data to speed things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=79.523160584230865, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.001766074650481071,\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85599999999999998"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty low but remember we only trained the model on 1,000 instances. Let's retrain the best estimator on the whole training set (run this at night, it will take hours):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=79.523160584230865, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.001766074650481071,\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_estimator_.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rnd_search_cv.best_estimator_.predict(X_train_scaled)\n",
    "accuracy_score(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, this looks good! Let's select this model. Now we can test it on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rnd_search_cv.best_estimator_.predict(X_test_scaled)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad, but apparently the model is overfitting slightly. It's tempting to tweak the hyperparameters a bit more (e.g. decreasing `C` and/or `gamma`), but we would run the risk of overfitting the test set. Other people have found that the hyperparameters `C=5` and `gamma=0.005` yield even better performance (over 98% accuracy). By running the randomized search for longer and on a larger part of the training set, you may be able to find this as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the California housing dataset train and tune up an SVM regressor pipeline. Is your best SVR pipeline statistically significantly better than randomforests?\n",
    "\n",
    "For information on the see [here](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html)\n",
    "\n",
    "* This dataset consists of 20,640 samples and 9 features.\n",
    "\n",
    "\n",
    "** NOTE** The solution is partially filled in. But you will need to fill in the missing parts and run all cells, report results and your conclusions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data and do EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the dataset using Scikit-Learn's `fetch_california_housing()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing(data_home='/Users/rohan.kayan/scikit_learn_data')\n",
    "X = housing[\"data\"]\n",
    "y = housing[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'California housing dataset.\\n\\nThe original database is available from StatLib\\n\\n    http://lib.stat.cmu.edu/\\n\\nThe data contains 20,640 observations on 9 variables.\\n\\nThis dataset contains the average house value as target variable\\nand the following input variables (features): average income,\\nhousing average age, average rooms, average bedrooms, population,\\naverage occupation, latitude, and longitude in that order.\\n\\nReferences\\n----------\\n\\nPace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\\nStatistics and Probability Letters, 33 (1997) 291-297.\\n\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MedInc',\n",
       " 'HouseAge',\n",
       " 'AveRooms',\n",
       " 'AveBedrms',\n",
       " 'Population',\n",
       " 'AveOccup',\n",
       " 'Latitude',\n",
       " 'Longitude']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJkAAANeCAYAAACxgN+pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiRJkiSptf8fps3VEwkLa48AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f48d0458d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.DataFrame(X, columns=housing.feature_names)\n",
    "df.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training set, the targets are hundreds of thousands of dollars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x000001F48022E2B0>]], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEICAYAAAC55kg0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f4800c0828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Target variable\n",
    "# Target variable has been scaled by $100,000\n",
    "pd.DataFrame(y).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split it into a training set and a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a baseline pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to scale the data and use pipeline. Add pipelines here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a simple `LinearSVR` first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
       "     random_state=42, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "lin_svr = LinearSVR(random_state=42)\n",
    "lin_svr.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it performs on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96128066532972722"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_pred = lin_svr.predict(X_train_scaled)\n",
    "mse = mean_squared_error(y_train, y_pred)\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the RMSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98044921608909819"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training set, the targets are tens of thousands of dollars. The RMSE gives a rough idea of the kind of error you should expect (with a higher weight for large errors): so with this model we can expect errors somewhere around $100,000. Not great. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune hyperparameters of SVR\n",
    "Let's see if we can do better with an RBF Kernel. We will use randomized search with cross validation to find the appropriate hyperparameter values for `C` and `gamma`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] C=38.4540118847, gamma=0.0796945481864 ..........................\n",
      "[CV] ........... C=38.4540118847, gamma=0.0796945481864, total=  13.6s\n",
      "[CV] C=38.4540118847, gamma=0.0796945481864 ..........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   15.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........... C=38.4540118847, gamma=0.0796945481864, total=  13.5s\n",
      "[CV] C=38.4540118847, gamma=0.0796945481864 ..........................\n",
      "[CV] ........... C=38.4540118847, gamma=0.0796945481864, total=  14.1s\n",
      "[CV] C=74.1993941811, gamma=0.0157513204998 ..........................\n",
      "[CV] ........... C=74.1993941811, gamma=0.0157513204998, total=  10.3s\n",
      "[CV] C=74.1993941811, gamma=0.0157513204998 ..........................\n",
      "[CV] ........... C=74.1993941811, gamma=0.0157513204998, total=  10.1s\n",
      "[CV] C=74.1993941811, gamma=0.0157513204998 ..........................\n",
      "[CV] ........... C=74.1993941811, gamma=0.0157513204998, total=   9.9s\n",
      "[CV] C=16.6018640442, gamma=0.00205111041884 .........................\n",
      "[CV] .......... C=16.6018640442, gamma=0.00205111041884, total=   5.4s\n",
      "[CV] C=16.6018640442, gamma=0.00205111041884 .........................\n",
      "[CV] .......... C=16.6018640442, gamma=0.00205111041884, total=   5.3s\n",
      "[CV] C=16.6018640442, gamma=0.00205111041884 .........................\n",
      "[CV] .......... C=16.6018640442, gamma=0.00205111041884, total=   5.4s\n",
      "[CV] C=6.80836121682, gamma=0.0539948440979 ..........................\n",
      "[CV] ........... C=6.80836121682, gamma=0.0539948440979, total=   6.1s\n",
      "[CV] C=6.80836121682, gamma=0.0539948440979 ..........................\n",
      "[CV] ........... C=6.80836121682, gamma=0.0539948440979, total=   6.2s\n",
      "[CV] C=6.80836121682, gamma=0.0539948440979 ..........................\n",
      "[CV] ........... C=6.80836121682, gamma=0.0539948440979, total=   6.2s\n",
      "[CV] C=61.1115011743, gamma=0.0260702475837 ..........................\n",
      "[CV] ........... C=61.1115011743, gamma=0.0260702475837, total=  10.6s\n",
      "[CV] C=61.1115011743, gamma=0.0260702475837 ..........................\n",
      "[CV] ........... C=61.1115011743, gamma=0.0260702475837, total=  10.9s\n",
      "[CV] C=61.1115011743, gamma=0.0260702475837 ..........................\n",
      "[CV] ........... C=61.1115011743, gamma=0.0260702475837, total=  10.7s\n",
      "[CV] C=3.05844942958, gamma=0.087060208783 ...........................\n",
      "[CV] ............ C=3.05844942958, gamma=0.087060208783, total=   5.5s\n",
      "[CV] C=3.05844942958, gamma=0.087060208783 ...........................\n",
      "[CV] ............ C=3.05844942958, gamma=0.087060208783, total=   5.6s\n",
      "[CV] C=3.05844942958, gamma=0.087060208783 ...........................\n",
      "[CV] ............ C=3.05844942958, gamma=0.087060208783, total=   5.7s\n",
      "[CV] C=84.24426408, gamma=0.00265875439833 ...........................\n",
      "[CV] ............ C=84.24426408, gamma=0.00265875439833, total=   6.6s\n",
      "[CV] C=84.24426408, gamma=0.00265875439833 ...........................\n",
      "[CV] ............ C=84.24426408, gamma=0.00265875439833, total=   6.6s\n",
      "[CV] C=84.24426408, gamma=0.00265875439833 ...........................\n",
      "[CV] ............ C=84.24426408, gamma=0.00265875439833, total=   6.5s\n",
      "[CV] C=19.1824967207, gamma=0.00232706770838 .........................\n",
      "[CV] .......... C=19.1824967207, gamma=0.00232706770838, total=   5.3s\n",
      "[CV] C=19.1824967207, gamma=0.00232706770838 .........................\n",
      "[CV] .......... C=19.1824967207, gamma=0.00232706770838, total=   5.6s\n",
      "[CV] C=19.1824967207, gamma=0.00232706770838 .........................\n",
      "[CV] .......... C=19.1824967207, gamma=0.00232706770838, total=   5.4s\n",
      "[CV] C=31.424224296, gamma=0.0112076062119 ...........................\n",
      "[CV] ............ C=31.424224296, gamma=0.0112076062119, total=   6.8s\n",
      "[CV] C=31.424224296, gamma=0.0112076062119 ...........................\n",
      "[CV] ............ C=31.424224296, gamma=0.0112076062119, total=   6.5s\n",
      "[CV] C=31.424224296, gamma=0.0112076062119 ...........................\n",
      "[CV] ............ C=31.424224296, gamma=0.0112076062119, total=   6.8s\n",
      "[CV] C=44.1945018642, gamma=0.00382347522468 .........................\n",
      "[CV] .......... C=44.1945018642, gamma=0.00382347522468, total=   6.3s\n",
      "[CV] C=44.1945018642, gamma=0.00382347522468 .........................\n",
      "[CV] .......... C=44.1945018642, gamma=0.00382347522468, total=   6.1s\n",
      "[CV] C=44.1945018642, gamma=0.00382347522468 .........................\n",
      "[CV] .......... C=44.1945018642, gamma=0.00382347522468, total=   6.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:  5.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score='raise',\n",
       "          estimator=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n",
       "          fit_params={}, iid=True, n_iter=10, n_jobs=1,\n",
       "          param_distributions={'gamma': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001F480269BA8>, 'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001F480264EB8>},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score=True, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import reciprocal, uniform\n",
    "\n",
    "param_distributions = {\"gamma\": reciprocal(0.001, 0.1), \"C\": uniform(1, 100)}\n",
    "rnd_search_cv = RandomizedSearchCV(SVR(), param_distributions, n_iter=10, verbose=2, random_state=42)\n",
    "rnd_search_cv.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=38.454011884736246, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
       "  gamma=0.079694548186439285, kernel='rbf', max_iter=-1, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's measure the RMSE on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5446663900096772"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rnd_search_cv.best_estimator_.predict(X_train_scaled)\n",
    "mse = mean_squared_error(y_train, y_pred)\n",
    "np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks much better than the linear model. Let's select this model and evaluate it on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rnd_search_cv.best_estimator_.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rnd_search_rmse = np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK: Contrast with a random forest with 100 decision trees with a depth of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import reciprocal, uniform, randint\n",
    "\n",
    "param_distributions = {\"n_estimators\": randint(low=1,high=200), \"max_features\": randint(low=1, high=8)}\n",
    "\n",
    "#rnd_search_cv = RandomizedSearchCV(RandomForestRegressor(), param_distributions, n_iter=10, verbose=2, random_state=42)\n",
    "#rnd_search_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators=100,max_depth=10)\n",
    "rfr.fit(X_train_scaled, y_train)\n",
    "\n",
    "#Random Forest Regressor \n",
    "y_pred = rfr.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse_rfr = np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54443164839561609"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_rfr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dtr = DecisionTreeRegressor(random_state=42, max_depth=10)\n",
    "dtr.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "y_pred = dtr.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse_dtr = np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean Squared error for Random Forest Regressor: 0.544431648396\n",
      "Root mean Squared error for Decision Tree Regressor: 0.645885901674\n",
      "Root mean Squared error for Random Search with SVR : 0.570463714768\n"
     ]
    }
   ],
   "source": [
    "print(\"Root mean Squared error for Random Forest Regressor:\",rmse_rfr)\n",
    "print(\"Root mean Squared error for Decision Tree Regressor:\",rmse_dtr)\n",
    "print(\"Root mean Squared error for Random Search with SVR :\",rnd_search_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regrssor is giving better results.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Optional Task] Extend Linear SVM classifier via BGD to multiple classes\n",
    "\n",
    "Extend the following implementation of a Linear SVM classifier via batch gradient descent to case there deals with 3 or more   classes.\n",
    "\n",
    "**NOTE** Adopt all the code and experiments in this section and modify as needed to classify the 3-class iris dataset on 2=dimensional input space (petal length, petal width).\n",
    "\n",
    "Please see lecture (video and slides for details of the multiple class training SVM algorithm) and read the next subsection `SVM Multiclass Loss function` to get more background. \n",
    "\n",
    "Please implement the ** vectorized version**  of multiclass Linear SVM.\n",
    "\n",
    "\n",
    "## SVM Multiclass Loss function\n",
    "Quality of weights is often expressed by a loss function, our unhappiness with classification result, and we want its value to be as small as possible. To minimize the loss, we have to define a loss function and find their partial derivatives with respect to the weights to update them iteratively.\n",
    "\n",
    "$$\\begin{equation}\n",
    "L_i = \\sum_{j\\neq y_i} \\left[ \\max(0, x_iw_j - x_iw_{y_i} + \\Delta) \\right] \\tag{1}\n",
    "\\end{equation} \n",
    "$$\n",
    "\n",
    "Where \n",
    "\n",
    "\n",
    "* $i$ iterates over all N examples,\n",
    "* $j$ iterates over all C classes,\n",
    "* $L_i$ is loss for classifying a single example  $x_i$ (row vector),\n",
    "* $w_j$ is the weights (column vector) for computing the score of class j,\n",
    "* $y_i$ is the index of the correct class of $x_i$, and\n",
    "* $Î”$ is a margin parameter which is commonly set to $1$\n",
    "\n",
    "Intuitively, SVM wants score, $x_iw_{y_i}$, of the correct class, $y_i$, to be greater than any other classes, $x_iw_j$, by at least $Î”$ such that the loss becomes zero (clamped with the max operation).\n",
    "\n",
    "### Analytic gradient\n",
    "The gradient of the loss function for a **single example**, $(\\vec{X_i}, y_i)$ can be written in full detail as:\n",
    "\n",
    "$$\\nabla_{w} L_i \n",
    "  =\n",
    "  \\begin{bmatrix}\n",
    "    \\frac{dL_i}{dw_1} & \\frac{dL_i}{dw_2} & \\cdots & \\frac{dL_i}{dw_C} \n",
    "  \\end{bmatrix}\n",
    "  = \n",
    "  \\begin{bmatrix}\n",
    "    \\frac{dL_i}{dw_{11}} & \\frac{dL_i}{dw_{21}} & \\cdots & \\frac{dL_i}{dw_{y_i1}} & \\cdots & \\frac{dL_i}{dw_{C1}} \\\\\n",
    "    \\vdots & \\ddots \\\\\n",
    "    \\frac{dL_i}{dw_{1D}} & \\frac{dL_i}{dw_{2D}} & \\cdots & \\frac{dL_i}{dw_{y_iD}} & \\cdots & \\frac{dL_i}{dw_{CD}} \n",
    "  \\end{bmatrix}\\tag{2}\n",
    "  $$\n",
    "  \n",
    "  First, letâ€™s find a sub-gradient $ \\frac{dL_i}{dw_{11}}$ (one element from equation (2) by considering all the terms in equation (1) (the SVM loss for one example $i$ with respect to all classes):\n",
    "  \n",
    " \n",
    "\\begin{align*}\n",
    "L_i = &\\max(0, x_{i1}w_{11} + x_{i2}w_{12} \\ldots + x_{iD}w_{1D} - x_{i1}w_{y_i1} - x_{i2}w_{y_i2} \\ldots - x_{iD}w_{y_iD} + \\Delta) + \\\\\n",
    " &\\max(0, x_{i1}w_{21} + x_{i2}w_{22} \\ldots + x_{iD}w_{2D} - x_{i1}w_{y_i1} - x_{i2}w_{y_i2} \\ldots - x_{iD}w_{y_iD} + \\Delta) + \\\\\n",
    "&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\vdots \\\\\n",
    "&\\max(0, x_{i1}w_{C1} + x_{i2}w_{C2} \\ldots + x_{iD}w_{CD} - x_{i1}w_{y_i1} - x_{i2}w_{y_i2} \\ldots - x_{iD}w_{y_iD} + \\Delta) \\tag{3}\n",
    "\\end{align*} \n",
    "\n",
    "For a general case, if $ (x_iw_1âˆ’x_iw_{y_i}+Î”)>0 $:\n",
    "\n",
    "$$\\frac{dL_i}{dw_{11}}=x_{i1} $$\n",
    "\n",
    "\n",
    "Looking closely at this equation: \n",
    "\n",
    "* $w_{11}$ corresponds to first weight in the weigth vector that is normal to the separtating hyperplance for  class 1 and \n",
    "* all terms in equation (3) that do not involve $w_{11}$ go to zero (think of them as a constant being  differentiated)\n",
    "\n",
    "Hence, we consider two cases:\n",
    "\n",
    "* $w_j$ for class weights of  the not class\n",
    "* $w_{y_i}$ for class weights of the  class \n",
    "\n",
    "\n",
    "#### Case 1: $w_j$ for class weights of  the not class\n",
    "\\begin{align*}\n",
    "\\frac{dL_i}{dw_{j}} &= \\mathbb{1}(x_iw_j - x_iw_{y_i} + \\Delta > 0)\n",
    "  \\begin{bmatrix}\n",
    "  x_{i1} \\\\\n",
    "  x_{i2} \\\\\n",
    "  \\vdots \\\\\n",
    "  x_{iD}\n",
    "  \\end{bmatrix}\n",
    "\\\\\n",
    "&= \\mathbb{1}(x_iw_j - x_iw_{y_i} + \\Delta > 0) x_i \\tag{4}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "Equivalently, using an indicator function of the form $1(.)$:\n",
    "\n",
    "$$ \\dfrac{dL_i}{dw_{11}} = 1(x_iw_1âˆ’x_iw_{y_i}+Î”>0)   = x_{i1} $$\n",
    "\n",
    "\n",
    "#### Case 1:$w_{y_i}$ for class weights of  the not class\n",
    "For a special case where $j=y_i$,\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{dL_i}{dw_{y_{i1}}} = -(\\ldots) x_{i1}\n",
    "$$\n",
    "\n",
    "\n",
    "The coefficent of $x_{i1}$ is the number of classes that meet the desire margin. Mathematically speaking, $\\sum_{j\\neq y_i} \\mathbb{1}(x_iw_j - x_iw_{y_i} + \\Delta > 0)$. Hence,\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{dL_i}{dw_{y_i}} &= - \\sum_{j\\neq y_i} \\mathbb{1}(x_iw_j - x_iw_{y_i} + \\Delta > 0)\n",
    "  \\begin{bmatrix}\n",
    "  x_{i1} \\\\\n",
    "  x_{i2} \\\\\n",
    "  \\vdots \\\\\n",
    "  x_{iD}\n",
    "  \\end{bmatrix}\n",
    "\\\\\n",
    "&= - \\sum_{j\\neq y_i} \\mathbb{1}(x_iw_j - x_iw_{y_i} + \\Delta > 0) x_i \\tag{5}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "<p>Equipped with equations (1)-(5), we have enough information to implement a loss function and gradient update for a multiclass SVM.</p>\n",
    "\n",
    "<p>The IPython Notebook <a href=\"http://vision.stanford.edu/teaching/cs231n/winter1516_assignment1.zip\">svm.ipynb</a> from <a href=\"http://vision.stanford.edu/teaching/cs231n/syllabus.html\">Stanford CS231n</a> is a great starting point to understand implementation of the SVM classifier. The exercise asks us to implement both non-vectorized and vectorized versions of loss function and gradient update.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### non-vectorized-implementation\n",
    "\n",
    "Looking at the terms in equation (1) suggests us to compute scores of all classes with $x_iW$, given an example $x_i$. The loss function can be implemented with two for-loops. The inner loop collects loss of all classes of a single example and the outer loop collects it across all examples. \n",
    "\n",
    "\n",
    "We compute analytic gradient \n",
    "$\\nabla_{w} L_i \n",
    "  =\n",
    "  \\begin{bmatrix}\n",
    "    \\frac{dL_i}{dw_1} & \\frac{dL_i}{dw_2} & \\cdots & \\frac{dL_i}{dw_C} \n",
    "  \\end{bmatrix}\n",
    "$  one element at a time in the inner loop. \n",
    "\n",
    "\n",
    "Considering equation (4) (CASE 1), we compute the gradient w.r.t. weights of class j with `dW[:,j] += X[i,:]`. Note that we use `+=` here as we have to collect the gradient across all classes j and across all examples. Considering equation (5)  (CASE 2), we compute the gradient of class $y_i$ with `dW[:,y[i]] -= X[i,:]`. Unlike the previous case, this single class $y_i$ requires us to count the number of classes that satisfy the margin condition; hence, the use of `-=`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svm_loss_naive(W, X, y, reg):\n",
    "  \"\"\"\n",
    "  Structured SVM loss function, naive implementation (with loops).\n",
    "\n",
    "  Inputs have dimension D, there are C classes, and we operate on minibatches\n",
    "  of N examples.\n",
    "\n",
    "  Inputs:\n",
    "  - W: A numpy array of shape (D, C) containing weights.\n",
    "  - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "  - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "    that X[i] has label c, where 0 <= c < C.\n",
    "  - reg: (float) regularization strength\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - loss as single float\n",
    "  - gradient with respect to weights W; an array of same shape as W\n",
    "  \"\"\"\n",
    "  dW = np.zeros(W.shape) # initialize the gradient as zero\n",
    "  \n",
    "  # compute the loss and the gradient\n",
    "  num_classes = W.shape[1]\n",
    "  num_train = X.shape[0]\n",
    "  loss = 0.0\n",
    "  for i in xrange(num_train):\n",
    "    scores = X[i,:].dot(W)\n",
    "    correct_class_score = scores[y[i]]\n",
    "    for j in xrange(num_classes):\n",
    "      if j == y[i]:\n",
    "        continue\n",
    "      margin = scores[j] - correct_class_score + 1 \n",
    "      if margin > 0:\n",
    "        loss += margin\n",
    "        dW[:,y[i]] -= X[i,:] \n",
    "        dW[:,j] += X[i,:] \n",
    "\n",
    "  # Averaging over all examples\n",
    "  loss /= num_train\n",
    "  dW /= num_train\n",
    "\n",
    "  # Add regularization\n",
    "  loss += 0.5 * reg * np.sum(W * W)\n",
    "  dW += reg*W\n",
    "  \n",
    "  return loss, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized implementation (partial)\n",
    "Instead of computing scores for each example, $x_iW$, we can compute them all at once with full matrix multiplication, $XW$. To compute the loss, this score matrix has to be subtracted row-wise by scores of correct classes and then added with Î”. Because the loss equation sums over all j except $y_i$, we have to set the yi component to zero. The trick to select correct-class scores across all examples is to use an array indexing technique together with NumPyâ€™s arange. Please take this idea and implement the vectorized version of a multiclass SVM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-def7b3b8974f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0miris\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_iris\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miris\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# petal length, petal width\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miris\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = iris.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class MyLinearSVC(BaseEstimator):\n",
    "    def __init__(self, C=1, eta0=1, eta_d=10000, n_epochs=1000, random_state=None):\n",
    "        self.C = C\n",
    "        self.eta0 = eta0\n",
    "        self.n_epochs = n_epochs\n",
    "        self.random_state = random_state\n",
    "        self.eta_d = eta_d\n",
    "\n",
    "    def eta(self, epoch):\n",
    "        return self.eta0 / (epoch + self.eta_d)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Random initialization\n",
    "        if self.random_state:\n",
    "            np.random.seed(self.random_state)\n",
    "        w = np.random.randn(X.shape[1], 1) # n feature weights\n",
    "        b = 0\n",
    "\n",
    "        m = len(X)\n",
    "        t = y * 2 - 1  # -1 if t==0, +1 if t==1\n",
    "        X_t = X * t\n",
    "        self.Js=[]\n",
    "\n",
    "        # Training\n",
    "        for epoch in range(self.n_epochs):\n",
    "            support_vectors_idx = (X_t.dot(w) + t * b < 1).ravel()\n",
    "            X_t_sv = X_t[support_vectors_idx]\n",
    "            t_sv = t[support_vectors_idx]\n",
    "\n",
    "            J = 1/2 * np.sum(w * w) + self.C * (np.sum(1 - X_t_sv.dot(w)) - b * np.sum(t_sv))\n",
    "            self.Js.append(J)\n",
    "\n",
    "            w_gradient_vector = w - self.C * np.sum(X_t_sv, axis=0).reshape(-1, 1)\n",
    "            b_derivative = -C * np.sum(t_sv)\n",
    "                \n",
    "            w = w - self.eta(epoch) * w_gradient_vector\n",
    "            b = b - self.eta(epoch) * b_derivative\n",
    "            \n",
    "\n",
    "        self.intercept_ = np.array([b])\n",
    "        self.coef_ = np.array([w])\n",
    "        support_vectors_idx = (X_t.dot(w) + b < 1).ravel()\n",
    "        self.support_vectors_ = X[support_vectors_idx]\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        return X.dot(self.coef_[0]) + self.intercept_[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.decision_function(X) >= 0).astype(np.float64)\n",
    "\n",
    "C=2\n",
    "svm_clf = MyLinearSVC(C=C, eta0 = 10, eta_d = 1000, n_epochs=60000, random_state=2)\n",
    "svm_clf.fit(X, y)\n",
    "svm_clf.predict(np.array([[5, 2], [4, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(svm_clf.n_epochs), svm_clf.Js)\n",
    "plt.axis([0, svm_clf.n_epochs, 0, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(svm_clf.intercept_, svm_clf.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_clf2 = SVC(kernel=\"linear\", C=C)\n",
    "svm_clf2.fit(X, y.ravel())\n",
    "print(svm_clf2.intercept_, svm_clf2.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yr = y.ravel()\n",
    "plt.figure(figsize=(12,3.2))\n",
    "plt.subplot(121)\n",
    "plt.plot(X[:, 0][yr==1], X[:, 1][yr==1], \"g^\", label=\"Iris-Virginica\")\n",
    "plt.plot(X[:, 0][yr==0], X[:, 1][yr==0], \"bs\", label=\"Not Iris-Virginica\")\n",
    "plot_svc_decision_boundary(svm_clf, 4, 6)\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.title(\"MyLinearSVC\", fontsize=14)\n",
    "plt.axis([4, 6, 0.8, 2.8])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(X[:, 0][yr==1], X[:, 1][yr==1], \"g^\")\n",
    "plt.plot(X[:, 0][yr==0], X[:, 1][yr==0], \"bs\")\n",
    "plot_svc_decision_boundary(svm_clf2, 4, 6)\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.title(\"SVC\", fontsize=14)\n",
    "plt.axis([4, 6, 0.8, 2.8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(loss=\"hinge\", alpha = 0.017, n_iter = 50, random_state=42)\n",
    "sgd_clf.fit(X, y.ravel())\n",
    "\n",
    "m = len(X)\n",
    "t = y * 2 - 1  # -1 if t==0, +1 if t==1\n",
    "X_b = np.c_[np.ones((m, 1)), X]  # Add bias input x0=1\n",
    "X_b_t = X_b * t\n",
    "sgd_theta = np.r_[sgd_clf.intercept_[0], sgd_clf.coef_[0]]\n",
    "print(sgd_theta)\n",
    "support_vectors_idx = (X_b_t.dot(sgd_theta) < 1).ravel()\n",
    "sgd_clf.support_vectors_ = X[support_vectors_idx]\n",
    "sgd_clf.C = C\n",
    "\n",
    "plt.figure(figsize=(5.5,3.2))\n",
    "plt.plot(X[:, 0][yr==1], X[:, 1][yr==1], \"g^\")\n",
    "plt.plot(X[:, 0][yr==0], X[:, 1][yr==0], \"bs\")\n",
    "plot_svc_decision_boundary(sgd_clf, 4, 6)\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.title(\"SGDClassifier\", fontsize=14)\n",
    "plt.axis([4, 6, 0.8, 2.8])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare homegrown with LinearSVC and SVC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise: train a `LinearSVC` on a linearly separable dataset. Then train an `SVC` and a `SGDClassifier` on the same dataset. See if you can get them to produce roughly the same model._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the Iris dataset: the Iris Setosa and Iris Versicolor classes are linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = iris[\"target\"]\n",
    "\n",
    "setosa_or_versicolor = (y == 0) | (y == 1)\n",
    "X = X[setosa_or_versicolor]\n",
    "y = y[setosa_or_versicolor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "C = 5\n",
    "alpha = 1 / (C * len(X))\n",
    "\n",
    "lin_clf = LinearSVC(loss=\"hinge\", C=C, random_state=42)\n",
    "svm_clf = SVC(kernel=\"linear\", C=C)\n",
    "sgd_clf = SGDClassifier(loss=\"hinge\", learning_rate=\"constant\", eta0=0.001, alpha=alpha,\n",
    "                        n_iter=100000, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "lin_clf.fit(X_scaled, y)\n",
    "svm_clf.fit(X_scaled, y)\n",
    "sgd_clf.fit(X_scaled, y)\n",
    "\n",
    "print(\"LinearSVC:                   \", lin_clf.intercept_, lin_clf.coef_)\n",
    "print(\"SVC:                         \", svm_clf.intercept_, svm_clf.coef_)\n",
    "print(\"SGDClassifier(alpha={:.5f}):\".format(sgd_clf.alpha), sgd_clf.intercept_, sgd_clf.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the decision boundaries of these three models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the slope and bias of each decision boundary\n",
    "w1 = -lin_clf.coef_[0, 0]/lin_clf.coef_[0, 1]\n",
    "b1 = -lin_clf.intercept_[0]/lin_clf.coef_[0, 1]\n",
    "w2 = -svm_clf.coef_[0, 0]/svm_clf.coef_[0, 1]\n",
    "b2 = -svm_clf.intercept_[0]/svm_clf.coef_[0, 1]\n",
    "w3 = -sgd_clf.coef_[0, 0]/sgd_clf.coef_[0, 1]\n",
    "b3 = -sgd_clf.intercept_[0]/sgd_clf.coef_[0, 1]\n",
    "\n",
    "# Transform the decision boundary lines back to the original scale\n",
    "line1 = scaler.inverse_transform([[-10, -10 * w1 + b1], [10, 10 * w1 + b1]])\n",
    "line2 = scaler.inverse_transform([[-10, -10 * w2 + b2], [10, 10 * w2 + b2]])\n",
    "line3 = scaler.inverse_transform([[-10, -10 * w3 + b3], [10, 10 * w3 + b3]])\n",
    "\n",
    "# Plot all three decision boundaries\n",
    "plt.figure(figsize=(11, 4))\n",
    "plt.plot(line1[:, 0], line1[:, 1], \"k:\", label=\"LinearSVC\")\n",
    "plt.plot(line2[:, 0], line2[:, 1], \"b--\", linewidth=2, label=\"SVC\")\n",
    "plt.plot(line3[:, 0], line3[:, 1], \"r-\", label=\"SGDClassifier\")\n",
    "plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\") # label=\"Iris-Versicolor\"\n",
    "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\") # label=\"Iris-Setosa\"\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(loc=\"upper center\", fontsize=14)\n",
    "plt.axis([0, 5.5, 0, 2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close enough!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "nav_menu": {},
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
